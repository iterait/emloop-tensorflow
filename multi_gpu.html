<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Multi-GPU models &#8212; cxflow-tensorflow 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/highlight.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Tutorial" href="tutorial.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          cxflow-tensorflow</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="getting_started.html">Getting Started</a></li>
                <li><a href="tutorial.html">Tutorial</a></li>
                <li><a href="#">Multi GPU models</a></li>
                <li><a href="cxflow_tensorflow/index.html">API Reference</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Pages <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary">
<div class="side_nav">
    <a href="tutorial.html" title="Previous Chapter: Tutorial"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Tutorial</span>
    </a>
    </div><ul>
<li><a class="reference internal" href="#">Multi-GPU models</a><ul>
<li><a class="reference internal" href="#compatible-models">Compatible models</a></li>
<li><a class="reference internal" href="#multi-gpu-training">Multi-GPU training</a></li>
<li><a class="reference internal" href="#implementation-details">Implementation details</a></li>
</ul>
</li>
</ul>

<form action="search.html" method="get" class="searchbox">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9 content">
      
  <div class="section" id="multi-gpu-models">
<h1>Multi-GPU models<a class="headerlink" href="#multi-gpu-models" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p>Training deep learning models often require large amounts of data and computation resources rendering the process
quite time expensive.
Models are trained for hours or days which makes experimenting with different configurations cumbersome and slow.</p>
<div class="figure align-right" id="id1">
<a class="reference internal image-reference" href="_images/multi-gpu-speed.png"><img alt="_images/multi-gpu-speed.png" src="_images/multi-gpu-speed.png" style="width: 491.20000000000005px; height: 222.4px;" /></a>
<p class="caption"><span class="caption-text">Performance gain with multiple GPUs in <strong>cxflow-tensorflow</strong> framework.</span></p>
</div>
<p>One way to speed up the process is utilizing multiple GPUs for model training. This may significantly lower the
wall clock time, yet, the implementation is not always straightforward. Lucky enough, we implemented the necessary
routines to make arbitrary TensorFlow model eligible for multi GPU training.</p>
<div class="section" id="compatible-models">
<h2>Compatible models<a class="headerlink" href="#compatible-models" title="Permalink to this headline">¶</a></h2>
<p>The chances are, your TensorFlow model is already ready for multi GPU training in <strong>cxflow-tensorflow</strong>.
In fact, the only requirement is to use <code class="docutils literal"><span class="pre">tf.get_variable</span></code> instead of <code class="docutils literal"><span class="pre">tf.Variable</span></code> so that the variables may be
reused and shared over multiple GPUs.</p>
</div>
<div class="section" id="multi-gpu-training">
<h2>Multi-GPU training<a class="headerlink" href="#multi-gpu-training" title="Permalink to this headline">¶</a></h2>
<p>To train on (multiple) GPUs, define the <code class="docutils literal"><span class="pre">n_gpus</span></code> parameter in the configuration file. E.g.:</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">config.yaml</span><a class="headerlink" href="#id2" title="Permalink to this code">¶</a></div>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">model</span><span class="p p-Indicator">:</span>
   <span class="l l-Scalar l-Scalar-Plain">class</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">nets.MyMultiGPUNet</span>
<span class="hll">   <span class="l l-Scalar l-Scalar-Plain">n_gpus</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
</span>   <span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
</div>
<p>That&#8217;s it! Now better check your server temperatures...</p>
<p>Fully working example may be found HERE.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Increase your dataset <strong>batch size</strong> together with the <code class="docutils literal"><span class="pre">n_gpus</span></code> parameter.
Each batch is distributed evenly between the GPUs alowing for much greater batch sizes!</p>
</div>
</div>
<div class="section" id="implementation-details">
<h2>Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<div class="figure align-right" id="id3">
<img alt="_images/multi-gpu-graph.png" src="_images/multi-gpu-graph.png" />
<p class="caption"><span class="caption-text">Tower architecture for multi GPU training.</span></p>
</div>
<p>In order to leverage the computation resources of multiple GPUs, <strong>cxflow-tensorflow</strong> creates a bit more complicated
computational graph than usual. In fact, the whole model is multiplicated so that each GPU has its own <em>tower</em>. The
variables to be trained are, naturally, shared between the towers. Each GPU computes its own feed-forward pass and the
respective variable gradients. Consequently, the gradients are averaged and used for update.</p>
<p>During the training <strong>cxflow-tensorflow</strong> distributes each batch equally among the towers. The fetched outputs are
concatenated preserving its orders which ensures perfect abstraction for the remaining <strong>cxflow</strong> components. From the
perspective of hooks and datasets, nothing has changed. Additionally, <strong>cxflow-tensorflow</strong> handles
incomplete batches with ease.</p>
<p>A model trained on multiple GPUs may be restored and used for inference with equal or less amount of GPUs or even
on CPU without any troubles.</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2017, Blazek Adam, Belohlavek Petr, Matzner Filip.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>